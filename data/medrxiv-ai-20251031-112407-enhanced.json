[
  {
    "title": "Multimodal Large Language Models for Automated Radiology Report Generation and Clinical Decision Support",
    "authors": [
      "Chen X",
      "Wang H",
      "Liu Y",
      "Zhang M"
    ],
    "corresponding_author": "Chen X",
    "affiliations": [
      "Johns Hopkins University",
      "Stanford AI Lab"
    ],
    "abstract": "\nBackground: Radiology report generation remains time-intensive, and diagnostic accuracy varies across expertise levels.\nWe developed a multimodal large language model (MLLM) integrating visual and textual information for automated \nradiology report generation with clinical decision support.\n\nMethods: We trained GPT-4V-based architecture on 450,000 paired chest X-rays and reports from 120 hospitals across \nthree continents (2020-2024). The model processes DICOM images and patient history to generate structured reports \nfollowing the ACR standardized template. We incorporated retrieval-augmented generation (RAG) to reference similar \nhistorical cases and current clinical guidelines. The system was evaluated against 50 board-certified radiologists \non 5,000 test cases using BLEU-4, clinical accuracy metrics, and time-to-diagnosis measurements.\n\nResults: The MLLM achieved 0.89 BLEU-4 score for report quality, with 94.2% accuracy for critical findings detection \n(95% CI: 93.1-95.3), compared to 92.8% for junior radiologists and 96.1% for senior radiologists. Sensitivity for \npneumothorax was 96.7%, specificity 98.2%. The model reduced average report generation time from 8.3 minutes to \n45 seconds while maintaining comparable accuracy. In a prospective clinical trial across 15 emergency departments, \nAI-assisted reporting reduced door-to-diagnosis time by 42% and improved inter-reader agreement (kappa 0.91 vs 0.78).\n\nConclusions: Multimodal LLMs can generate high-quality radiology reports approaching senior radiologist performance, \nwith potential to improve clinical workflow efficiency and diagnostic consistency in resource-limited settings.\n        ",
    "url": "https://www.medrxiv.org/content/10.1101/2025.01.08.25301234",
    "published_at": "2025-01-08T00:00:00",
    "source": "medrxiv_2025",
    "doi": "10.1101/2025.01.08.25301234",
    "what_done": "开发并验证了基于GPT-4V的多模态大语言模型，该模型处理胸部X光图像和患者病史以自动生成结构化放射学报告。模型在来自120家医院的45万对配对图像和报告上进行训练，结合检索增强生成技术参考相似病例和临床指南，并在5000个测试病例上与50名执业放射科医师进行了对比评估。",
    "ai_role": "多模态大语言模型作为自动放射学报告生成系统提供临床决策支持，处理医学图像和文本以生成标准化诊断报告。该系统在关键发现检测方面达到94.2%的准确率，将报告生成时间从8.3分钟缩短至45秒，同时保持与经验丰富的放射科医师相当的诊断质量。",
    "models": "GPT-4V (multimodal large language model), retrieval-augmented generation (RAG)",
    "data_sources": "450,000 paired chest X-rays and radiology reports from 120 hospitals across three continents (2020-2024), 5,000 test cases",
    "metrics": "BLEU-4 score 0.89, accuracy 94.2% (95% CI: 93.1-95.3), sensitivity 96.7% for pneumothorax, specificity 98.2%, inter-reader agreement kappa 0.91, time reduction from 8.3 minutes to 45 seconds, 42% reduction in door-to-diagnosis time",
    "needs_manual_review": false
  },
  {
    "title": "Foundation Models for Protein Structure Prediction: AlphaFold 3 Validation in Drug Discovery Pipeline",
    "authors": [
      "Kumar R",
      "Thompson S",
      "Patel N",
      "Anderson K"
    ],
    "corresponding_author": "Kumar R",
    "affiliations": [
      "MIT CSAIL",
      "Broad Institute",
      "Pfizer Research"
    ],
    "abstract": "\nObjective: To validate AlphaFold 3's protein structure predictions in a real-world drug discovery pipeline and \nassess its impact on hit-to-lead optimization timelines.\n\nMethods: We integrated AlphaFold 3 into our structure-based drug design workflow for three therapeutic targets: \nKRAS G12C (oncology), PCSK9 (cardiovascular), and SARS-CoV-2 main protease. The foundation model predicted \nprotein-ligand complexes for 2,847 compounds, which were validated through X-ray crystallography (n=124) and \ncryo-EM (n=38). We compared prediction accuracy, computational cost, and drug optimization outcomes against \ntraditional homology modeling and molecular dynamics approaches. A retrospective analysis examined 18 completed \ndrug discovery programs from 2020-2024.\n\nResults: AlphaFold 3 achieved 1.8Å median RMSD for backbone prediction and 2.4Å for ligand binding poses, \nrepresenting 34% improvement over previous methods. Prediction confidence scores (pLDDT >90) showed 97% correlation \nwith experimental validation. In prospective drug optimization, AI-guided designs showed 2.3× higher success rate \nin lead optimization (45% vs 19%, p<0.001) and reduced average optimization cycles from 4.2 to 2.7. The foundation \nmodel reduced computational time from 72 hours (MD simulations) to 15 minutes while using 95% less computational \nresources. Integration into the pipeline accelerated preclinical candidate identification by 8-14 months.\n\nConclusions: Foundation models for protein structure enable accurate, rapid predictions that significantly \naccelerate drug discovery timelines and reduce computational costs, with performance approaching experimental methods.\n        ",
    "url": "https://www.medrxiv.org/content/10.1101/2025.02.14.25302567",
    "published_at": "2025-02-14T00:00:00",
    "source": "medrxiv_2025",
    "doi": "10.1101/2025.02.14.25302567",
    "what_done": "在真实药物发现流程中验证了AlphaFold 3的蛋白质结构预测能力，将该基础模型整合到三个治疗靶点（KRAS G12C、PCSK9、SARS-CoV-2蛋白酶）的基于结构的药物设计工作流程中。对2847个化合物的蛋白质-配体复合物进行预测，并通过X射线晶体学(n=124)和冷冻电镜(n=38)验证预测结果，与传统方法比较准确性和效率。",
    "ai_role": "AlphaFold 3基础模型执行快速、准确的蛋白质结构和蛋白质-配体结合预测以指导药物设计决策。该AI替代了计算成本高昂的分子动力学模拟，将预测时间从72小时缩短至15分钟，同时将准确率提高34%，并将临床前候选药物鉴定加速8-14个月。",
    "models": "AlphaFold 3 (foundation model for protein structure prediction)",
    "data_sources": "2,847 compounds for three therapeutic targets (KRAS G12C, PCSK9, SARS-CoV-2 protease), validation with 124 X-ray crystallography structures and 38 cryo-EM structures, retrospective analysis of 18 drug discovery programs (2020-2024)",
    "metrics": "1.8Å median RMSD for backbone prediction, 2.4Å for ligand binding poses (34% improvement), pLDDT >90 with 97% correlation to experimental validation, 2.3× higher success rate in lead optimization (45% vs 19%, p<0.001), reduced optimization cycles from 4.2 to 2.7, 95% reduction in computational resources",
    "needs_manual_review": false
  },
  {
    "title": "Real-World Implementation of AI-Powered Sepsis Prediction in ICUs: Multi-Center Randomized Controlled Trial",
    "authors": [
      "Martinez A",
      "Johnson M",
      "Williams R",
      "Brown L",
      "Davis K"
    ],
    "corresponding_author": "Martinez A",
    "affiliations": [
      "Mayo Clinic",
      "Cleveland Clinic",
      "Massachusetts General Hospital"
    ],
    "abstract": "\nBackground: Sepsis remains a leading cause of hospital mortality with time-critical treatment windows. We conducted \na multi-center RCT evaluating an AI-powered early warning system for sepsis prediction in intensive care units.\n\nMethods: This pragmatic RCT enrolled 12,450 ICU patients across 24 hospitals (June 2024-January 2025). The intervention \narm received real-time sepsis predictions from a transformer-based deep learning model analyzing 147 clinical variables \nfrom EHR streams (vital signs, labs, medications, nursing notes). The model was trained on 380,000 ICU admissions using \ntemporal convolutional networks with attention mechanisms, predicting sepsis onset 6-12 hours before clinical criteria. \nControl arm received standard care. Primary outcome was 28-day mortality; secondary outcomes included time-to-antibiotic \nadministration, ICU length of stay, and cost-effectiveness.\n\nResults: The AI system achieved AUROC 0.94 (95% CI: 0.93-0.95) for 6-hour ahead prediction with 87% sensitivity at \n90% specificity. In the intervention arm, 28-day mortality was reduced from 14.2% to 11.3% (absolute risk reduction \n2.9%, p=0.002, NNT=34). Median time from sepsis onset to antibiotic administration decreased from 3.2 to 1.8 hours \n(p<0.001). Early detection enabled 24% reduction in median ICU length of stay (5.2 vs 6.8 days) and $8,400 lower \nper-patient costs. The system showed consistent performance across diverse patient populations and hospital settings \nwith minimal performance degradation (AUROC variation 0.91-0.96). Alert fatigue was manageable with 12% false positive \nrate and positive predictive value of 31%.\n\nConclusions: AI-powered sepsis prediction integrated into clinical workflows significantly reduces mortality and \nhealthcare costs, demonstrating the potential of machine learning for real-time clinical decision support in \ncritical care settings.\n        ",
    "url": "https://www.medrxiv.org/content/10.1101/2025.03.22.25303891",
    "published_at": "2025-03-22T00:00:00",
    "source": "medrxiv_2025",
    "doi": "10.1101/2025.03.22.25303891",
    "what_done": "在24家医院开展多中心随机对照试验，纳入12450名ICU患者以评估AI驱动的脓毒症早期预警系统。开发了基于Transformer的深度学习模型，在38万次ICU入院数据上训练，分析来自电子病历系统的147个实时临床变量，以在临床标准出现前6-12小时预测脓毒症发作。",
    "ai_role": "基于Transformer的AI系统提供实时脓毒症风险预测以实现早期干预，分析生命体征、实验室结果、用药和护理记录的连续数据流。该系统实现了提前6小时预测且AUROC达0.94，将28天死亡率从14.2%降至11.3%，并将抗生素给药时间从3.2小时缩短至1.8小时。",
    "models": "Transformer-based deep learning with temporal convolutional networks and attention mechanisms",
    "data_sources": "Training: 380,000 ICU admissions; RCT: 12,450 ICU patients across 24 hospitals (June 2024-January 2025), 147 real-time clinical variables from EHR streams",
    "metrics": "AUROC 0.94 (95% CI: 0.93-0.95) for 6-hour prediction, 87% sensitivity at 90% specificity, 28-day mortality reduction from 14.2% to 11.3% (ARR 2.9%, p=0.002, NNT=34), time to antibiotics reduced from 3.2 to 1.8 hours, 24% reduction in ICU length of stay, $8,400 cost savings per patient, PPV 31%, false positive rate 12%",
    "needs_manual_review": false
  },
  {
    "title": "Federated Learning for Privacy-Preserving Genomic Analysis: A Multi-Institutional Pediatric Cancer Study",
    "authors": [
      "Lee J",
      "Park S",
      "Kim H",
      "Choi Y",
      "Song M"
    ],
    "corresponding_author": "Lee J",
    "affiliations": [
      "Seoul National University Hospital",
      "Stanford Medicine",
      "Children's Hospital of Philadelphia"
    ],
    "abstract": "\nPurpose: Pediatric cancer genomics requires large-scale multi-institutional collaboration, but data sharing is \nconstrained by privacy regulations and institutional policies. We implemented federated learning to enable \ncollaborative genomic analysis without sharing raw patient data.\n\nMethods: We established a federated learning network across 42 pediatric oncology centers in 18 countries, \nencompassing 28,500 pediatric cancer cases with whole-genome sequencing (WGS) and clinical outcomes data. Using \nsecure multi-party computation and differential privacy (ε=1.2), we trained deep neural networks for cancer subtype \nclassification, treatment response prediction, and survival analysis. The architecture employed graph neural networks \nto capture gene interaction patterns and attention mechanisms for variant prioritization. Each institution maintained \nlocal data control while contributing encrypted model updates. We compared federated model performance against \ncentralized training on publicly available datasets and single-institution models.\n\nResults: The federated model achieved 92.7% accuracy for cancer subtype classification across 23 disease categories, \ncomparable to centralized training (93.1%, p=0.24) and substantially better than average single-institution performance \n(78.3%, p<0.001). For treatment response prediction, the model reached AUROC 0.88, identifying 847 novel \ngenotype-phenotype associations including 23 actionable therapeutic targets. Survival prediction showed concordance \nindex of 0.81. The federated approach preserved privacy with formal differential privacy guarantees while enabling \nanalysis of rare disease subtypes impossible at single institutions. Model training required 6.2 days across the \nnetwork with 18TB of aggregate data processed locally, versus 14 months for traditional IRB approvals and data \ntransfers in historical multi-site studies.\n\nConclusions: Federated learning enables large-scale genomic research while preserving patient privacy and institutional \ndata sovereignty, offering a scalable solution for collaborative precision medicine in rare diseases.\n        ",
    "url": "https://www.medrxiv.org/content/10.1101/2025.05.10.25305432",
    "published_at": "2025-05-10T00:00:00",
    "source": "medrxiv_2025",
    "doi": "10.1101/2025.05.10.25305432",
    "what_done": "在18个国家的42个儿科肿瘤中心建立联邦学习网络，在不共享原始患者数据的情况下对28500例儿科癌症病例的全基因组测序数据进行协作基因组分析。使用安全多方计算和差分隐私(ε=1.2)训练深度神经网络进行癌症亚型分类、治疗反应预测和生存分析，同时保持机构数据主权。",
    "ai_role": "联邦学习实现了保护隐私的协作AI模型训练，各机构贡献加密的模型更新而非共享敏感基因组数据。该方法允许分析单个机构无法完成的罕见儿科癌症，同时保持正式的差分隐私保证，发现了847个新的基因型-表型关联，包括23个可操作的治疗靶点。",
    "models": "Graph neural networks with attention mechanisms, secure multi-party computation, differential privacy (ε=1.2)",
    "data_sources": "28,500 pediatric cancer cases with whole-genome sequencing from 42 centers in 18 countries, 18TB aggregate data processed locally",
    "metrics": "92.7% accuracy for cancer subtype classification (vs 93.1% centralized, p=0.24; vs 78.3% single-institution, p<0.001), AUROC 0.88 for treatment response, concordance index 0.81 for survival, 847 novel associations discovered including 23 actionable targets, 6.2 days training time vs 14 months for traditional multi-site approvals",
    "needs_manual_review": false
  },
  {
    "title": "Vision-Language Models for Automated Surgical Skill Assessment and Real-Time Feedback in Robotic Surgery",
    "authors": [
      "Zhang Q",
      "Wu T",
      "Chen L",
      "Wang F"
    ],
    "corresponding_author": "Zhang Q",
    "affiliations": [
      "Harvard Medical School",
      "MIT Media Lab",
      "Intuitive Surgical"
    ],
    "abstract": "\nBackground: Surgical skill assessment remains subjective and labor-intensive, limiting feedback during training. \nWe developed vision-language models for automated, objective surgical skill evaluation with real-time feedback \nduring robotic-assisted procedures.\n\nMethods: We collected 12,000 hours of da Vinci surgical system recordings across 6,800 procedures (cholecystectomy, \nprostatectomy, hysterectomy) performed by surgeons ranging from novices (n=45) to experts (n=120). We trained a \nmultimodal transformer architecture combining surgical video analysis (CLIP-based visual encoder) with procedural \nlanguage understanding (GPT-4 based text encoder) to assess technical skills across 12 dimensions including instrument \nhandling, tissue manipulation, and time-motion efficiency. The model generates natural language feedback aligned with \nOSATS (Objective Structured Assessment of Technical Skills) criteria. We validated against expert ratings (3 blinded \nsurgical educators per case) and correlated with clinical outcomes across 2,400 independent test procedures.\n\nResults: The vision-language model achieved 0.89 correlation with expert consensus scores (Spearman's ρ, 95% CI: \n0.87-0.91) across all skill dimensions, with 94% agreement on pass/fail decisions. The model detected critical safety \nevents with 96.4% sensitivity (tissue trauma, bleeding, instrument collisions) and provided contextualized feedback \nwithin 200ms latency enabling real-time intervention. Automated assessment scores predicted postoperative complications \n(AUROC 0.82), operative time efficiency (r=0.76), and surgeon experience level with 91% accuracy. In a pilot training \nprogram with 60 surgical residents, AI-powered feedback accelerated skill acquisition by 34% (p<0.001) measured by \ntime to proficiency, and improved technical skill scores by 2.8 points (5-point scale) compared to traditional \nmentorship alone.\n\nConclusions: Vision-language AI models enable objective, scalable surgical skill assessment with real-time feedback, \npotentially transforming surgical education and quality assurance in the era of robotic surgery.\n        ",
    "url": "https://www.medrxiv.org/content/10.1101/2025.08.15.25307123",
    "published_at": "2025-08-15T00:00:00",
    "source": "medrxiv_2025",
    "doi": "10.1101/2025.08.15.25307123",
    "what_done": "开发并验证了结合手术视频分析（基于CLIP）和程序语言理解（基于GPT-4）的多模态Transformer架构，用于自动评估12个维度的技术手术技能。在从新手到专家外科医生完成的6800例手术的12000小时达芬奇手术录像上进行训练，并通过专家评分验证且与2400例独立测试手术的临床结果相关联。",
    "ai_role": "视觉-语言AI模型在机器人手术过程中提供自动化、客观的手术技能评估和实时自然语言反馈。它分析手术视频以评估符合OSATS标准的技术技能（器械操作、组织处理、效率），检测关键安全事件，并在200毫秒延迟内生成情境化反馈以实现实时干预和加速手术培训。",
    "models": "Multimodal transformer architecture with CLIP-based visual encoder and GPT-4 based text encoder",
    "data_sources": "12,000 hours of da Vinci surgical recordings from 6,800 procedures (cholecystectomy, prostatectomy, hysterectomy), 45 novice to 120 expert surgeons, validation on 2,400 independent procedures with expert ratings",
    "metrics": "0.89 correlation with expert consensus (Spearman's ρ, 95% CI: 0.87-0.91), 94% pass/fail agreement, 96.4% sensitivity for critical safety events, 200ms latency for real-time feedback, AUROC 0.82 for complication prediction, r=0.76 for operative efficiency, 91% accuracy for experience level, 34% faster skill acquisition (p<0.001), 2.8-point improvement on 5-point skill scale",
    "needs_manual_review": false
  }
]