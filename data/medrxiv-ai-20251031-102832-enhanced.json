[
  {
    "title": "Multimodal Large Language Models for Automated Radiology Report Generation and Clinical Decision Support",
    "authors": [
      "Chen X",
      "Wang H",
      "Liu Y",
      "Zhang M"
    ],
    "corresponding_author": "Chen X",
    "affiliations": [
      "Johns Hopkins University",
      "Stanford AI Lab"
    ],
    "abstract": "\nBackground: Radiology report generation remains time-intensive, and diagnostic accuracy varies across expertise levels.\nWe developed a multimodal large language model (MLLM) integrating visual and textual information for automated \nradiology report generation with clinical decision support.\n\nMethods: We trained GPT-4V-based architecture on 450,000 paired chest X-rays and reports from 120 hospitals across \nthree continents (2020-2024). The model processes DICOM images and patient history to generate structured reports \nfollowing the ACR standardized template. We incorporated retrieval-augmented generation (RAG) to reference similar \nhistorical cases and current clinical guidelines. The system was evaluated against 50 board-certified radiologists \non 5,000 test cases using BLEU-4, clinical accuracy metrics, and time-to-diagnosis measurements.\n\nResults: The MLLM achieved 0.89 BLEU-4 score for report quality, with 94.2% accuracy for critical findings detection \n(95% CI: 93.1-95.3), compared to 92.8% for junior radiologists and 96.1% for senior radiologists. Sensitivity for \npneumothorax was 96.7%, specificity 98.2%. The model reduced average report generation time from 8.3 minutes to \n45 seconds while maintaining comparable accuracy. In a prospective clinical trial across 15 emergency departments, \nAI-assisted reporting reduced door-to-diagnosis time by 42% and improved inter-reader agreement (kappa 0.91 vs 0.78).\n\nConclusions: Multimodal LLMs can generate high-quality radiology reports approaching senior radiologist performance, \nwith potential to improve clinical workflow efficiency and diagnostic consistency in resource-limited settings.\n        ",
    "url": "https://www.medrxiv.org/content/10.1101/2025.01.08.25301234",
    "published_at": "2025-01-08T00:00:00",
    "source": "medrxiv_2025",
    "doi": "10.1101/2025.01.08.25301234",
    "what_done": "Developed and validated a GPT-4V-based multimodal large language model that processes chest X-ray images and patient history to automatically generate structured radiology reports. The model was trained on 450,000 paired images and reports from 120 hospitals, incorporating retrieval-augmented generation for referencing similar cases and clinical guidelines, and evaluated against 50 board-certified radiologists on 5,000 test cases.",
    "ai_role": "The multimodal LLM serves as an automated radiology report generation system with clinical decision support, processing medical images and text to produce standardized diagnostic reports. It achieved 94.2% accuracy for critical findings detection and reduced report generation time from 8.3 minutes to 45 seconds while maintaining diagnostic quality comparable to experienced radiologists.",
    "models": "GPT-4V (multimodal large language model), retrieval-augmented generation (RAG)",
    "data_sources": "450,000 paired chest X-rays and radiology reports from 120 hospitals across three continents (2020-2024), 5,000 test cases",
    "metrics": "BLEU-4 score 0.89, accuracy 94.2% (95% CI: 93.1-95.3), sensitivity 96.7% for pneumothorax, specificity 98.2%, inter-reader agreement kappa 0.91, time reduction from 8.3 minutes to 45 seconds, 42% reduction in door-to-diagnosis time",
    "needs_manual_review": false
  },
  {
    "title": "Foundation Models for Protein Structure Prediction: AlphaFold 3 Validation in Drug Discovery Pipeline",
    "authors": [
      "Kumar R",
      "Thompson S",
      "Patel N",
      "Anderson K"
    ],
    "corresponding_author": "Kumar R",
    "affiliations": [
      "MIT CSAIL",
      "Broad Institute",
      "Pfizer Research"
    ],
    "abstract": "\nObjective: To validate AlphaFold 3's protein structure predictions in a real-world drug discovery pipeline and \nassess its impact on hit-to-lead optimization timelines.\n\nMethods: We integrated AlphaFold 3 into our structure-based drug design workflow for three therapeutic targets: \nKRAS G12C (oncology), PCSK9 (cardiovascular), and SARS-CoV-2 main protease. The foundation model predicted \nprotein-ligand complexes for 2,847 compounds, which were validated through X-ray crystallography (n=124) and \ncryo-EM (n=38). We compared prediction accuracy, computational cost, and drug optimization outcomes against \ntraditional homology modeling and molecular dynamics approaches. A retrospective analysis examined 18 completed \ndrug discovery programs from 2020-2024.\n\nResults: AlphaFold 3 achieved 1.8Å median RMSD for backbone prediction and 2.4Å for ligand binding poses, \nrepresenting 34% improvement over previous methods. Prediction confidence scores (pLDDT >90) showed 97% correlation \nwith experimental validation. In prospective drug optimization, AI-guided designs showed 2.3× higher success rate \nin lead optimization (45% vs 19%, p<0.001) and reduced average optimization cycles from 4.2 to 2.7. The foundation \nmodel reduced computational time from 72 hours (MD simulations) to 15 minutes while using 95% less computational \nresources. Integration into the pipeline accelerated preclinical candidate identification by 8-14 months.\n\nConclusions: Foundation models for protein structure enable accurate, rapid predictions that significantly \naccelerate drug discovery timelines and reduce computational costs, with performance approaching experimental methods.\n        ",
    "url": "https://www.medrxiv.org/content/10.1101/2025.02.14.25302567",
    "published_at": "2025-02-14T00:00:00",
    "source": "medrxiv_2025",
    "doi": "10.1101/2025.02.14.25302567",
    "what_done": "Validated AlphaFold 3's protein structure predictions in real-world drug discovery by integrating the foundation model into structure-based drug design workflows for three therapeutic targets (KRAS G12C, PCSK9, SARS-CoV-2 protease). Predicted protein-ligand complexes for 2,847 compounds and validated predictions through X-ray crystallography (n=124) and cryo-EM (n=38), comparing accuracy and efficiency against traditional methods.",
    "ai_role": "AlphaFold 3 foundation model performs rapid, accurate protein structure and protein-ligand binding prediction to guide drug design decisions. The AI replaced computationally expensive molecular dynamics simulations, reducing prediction time from 72 hours to 15 minutes while improving accuracy by 34% and accelerating preclinical candidate identification by 8-14 months.",
    "models": "AlphaFold 3 (foundation model for protein structure prediction)",
    "data_sources": "2,847 compounds for three therapeutic targets (KRAS G12C, PCSK9, SARS-CoV-2 protease), validation with 124 X-ray crystallography structures and 38 cryo-EM structures, retrospective analysis of 18 drug discovery programs (2020-2024)",
    "metrics": "1.8Å median RMSD for backbone prediction, 2.4Å for ligand binding poses (34% improvement), pLDDT >90 with 97% correlation to experimental validation, 2.3× higher success rate in lead optimization (45% vs 19%, p<0.001), reduced optimization cycles from 4.2 to 2.7, 95% reduction in computational resources",
    "needs_manual_review": false
  },
  {
    "title": "Real-World Implementation of AI-Powered Sepsis Prediction in ICUs: Multi-Center Randomized Controlled Trial",
    "authors": [
      "Martinez A",
      "Johnson M",
      "Williams R",
      "Brown L",
      "Davis K"
    ],
    "corresponding_author": "Martinez A",
    "affiliations": [
      "Mayo Clinic",
      "Cleveland Clinic",
      "Massachusetts General Hospital"
    ],
    "abstract": "\nBackground: Sepsis remains a leading cause of hospital mortality with time-critical treatment windows. We conducted \na multi-center RCT evaluating an AI-powered early warning system for sepsis prediction in intensive care units.\n\nMethods: This pragmatic RCT enrolled 12,450 ICU patients across 24 hospitals (June 2024-January 2025). The intervention \narm received real-time sepsis predictions from a transformer-based deep learning model analyzing 147 clinical variables \nfrom EHR streams (vital signs, labs, medications, nursing notes). The model was trained on 380,000 ICU admissions using \ntemporal convolutional networks with attention mechanisms, predicting sepsis onset 6-12 hours before clinical criteria. \nControl arm received standard care. Primary outcome was 28-day mortality; secondary outcomes included time-to-antibiotic \nadministration, ICU length of stay, and cost-effectiveness.\n\nResults: The AI system achieved AUROC 0.94 (95% CI: 0.93-0.95) for 6-hour ahead prediction with 87% sensitivity at \n90% specificity. In the intervention arm, 28-day mortality was reduced from 14.2% to 11.3% (absolute risk reduction \n2.9%, p=0.002, NNT=34). Median time from sepsis onset to antibiotic administration decreased from 3.2 to 1.8 hours \n(p<0.001). Early detection enabled 24% reduction in median ICU length of stay (5.2 vs 6.8 days) and $8,400 lower \nper-patient costs. The system showed consistent performance across diverse patient populations and hospital settings \nwith minimal performance degradation (AUROC variation 0.91-0.96). Alert fatigue was manageable with 12% false positive \nrate and positive predictive value of 31%.\n\nConclusions: AI-powered sepsis prediction integrated into clinical workflows significantly reduces mortality and \nhealthcare costs, demonstrating the potential of machine learning for real-time clinical decision support in \ncritical care settings.\n        ",
    "url": "https://www.medrxiv.org/content/10.1101/2025.03.22.25303891",
    "published_at": "2025-03-22T00:00:00",
    "source": "medrxiv_2025",
    "doi": "10.1101/2025.03.22.25303891",
    "what_done": "Conducted a multi-center randomized controlled trial across 24 hospitals enrolling 12,450 ICU patients to evaluate an AI-powered sepsis early warning system. Developed a transformer-based deep learning model trained on 380,000 ICU admissions that analyzes 147 real-time clinical variables from EHR streams to predict sepsis onset 6-12 hours before clinical criteria are met.",
    "ai_role": "The transformer-based AI system provides real-time sepsis risk prediction to enable early intervention, analyzing continuous streams of vital signs, lab results, medications, and nursing notes. The system achieved 6-hour ahead prediction with AUROC 0.94, reducing 28-day mortality from 14.2% to 11.3% and decreasing time to antibiotic administration from 3.2 to 1.8 hours in the intervention arm.",
    "models": "Transformer-based deep learning with temporal convolutional networks and attention mechanisms",
    "data_sources": "Training: 380,000 ICU admissions; RCT: 12,450 ICU patients across 24 hospitals (June 2024-January 2025), 147 real-time clinical variables from EHR streams",
    "metrics": "AUROC 0.94 (95% CI: 0.93-0.95) for 6-hour prediction, 87% sensitivity at 90% specificity, 28-day mortality reduction from 14.2% to 11.3% (ARR 2.9%, p=0.002, NNT=34), time to antibiotics reduced from 3.2 to 1.8 hours, 24% reduction in ICU length of stay, $8,400 cost savings per patient, PPV 31%, false positive rate 12%",
    "needs_manual_review": false
  },
  {
    "title": "Federated Learning for Privacy-Preserving Genomic Analysis: A Multi-Institutional Pediatric Cancer Study",
    "authors": [
      "Lee J",
      "Park S",
      "Kim H",
      "Choi Y",
      "Song M"
    ],
    "corresponding_author": "Lee J",
    "affiliations": [
      "Seoul National University Hospital",
      "Stanford Medicine",
      "Children's Hospital of Philadelphia"
    ],
    "abstract": "\nPurpose: Pediatric cancer genomics requires large-scale multi-institutional collaboration, but data sharing is \nconstrained by privacy regulations and institutional policies. We implemented federated learning to enable \ncollaborative genomic analysis without sharing raw patient data.\n\nMethods: We established a federated learning network across 42 pediatric oncology centers in 18 countries, \nencompassing 28,500 pediatric cancer cases with whole-genome sequencing (WGS) and clinical outcomes data. Using \nsecure multi-party computation and differential privacy (ε=1.2), we trained deep neural networks for cancer subtype \nclassification, treatment response prediction, and survival analysis. The architecture employed graph neural networks \nto capture gene interaction patterns and attention mechanisms for variant prioritization. Each institution maintained \nlocal data control while contributing encrypted model updates. We compared federated model performance against \ncentralized training on publicly available datasets and single-institution models.\n\nResults: The federated model achieved 92.7% accuracy for cancer subtype classification across 23 disease categories, \ncomparable to centralized training (93.1%, p=0.24) and substantially better than average single-institution performance \n(78.3%, p<0.001). For treatment response prediction, the model reached AUROC 0.88, identifying 847 novel \ngenotype-phenotype associations including 23 actionable therapeutic targets. Survival prediction showed concordance \nindex of 0.81. The federated approach preserved privacy with formal differential privacy guarantees while enabling \nanalysis of rare disease subtypes impossible at single institutions. Model training required 6.2 days across the \nnetwork with 18TB of aggregate data processed locally, versus 14 months for traditional IRB approvals and data \ntransfers in historical multi-site studies.\n\nConclusions: Federated learning enables large-scale genomic research while preserving patient privacy and institutional \ndata sovereignty, offering a scalable solution for collaborative precision medicine in rare diseases.\n        ",
    "url": "https://www.medrxiv.org/content/10.1101/2025.05.10.25305432",
    "published_at": "2025-05-10T00:00:00",
    "source": "medrxiv_2025",
    "doi": "10.1101/2025.05.10.25305432",
    "what_done": "Implemented a federated learning network across 42 pediatric oncology centers in 18 countries to enable collaborative genomic analysis of 28,500 pediatric cancer cases with whole-genome sequencing data, without sharing raw patient data. Trained deep neural networks using secure multi-party computation and differential privacy (ε=1.2) for cancer subtype classification, treatment response prediction, and survival analysis while maintaining institutional data sovereignty.",
    "ai_role": "Federated learning enables privacy-preserving collaborative AI model training where institutions contribute encrypted model updates rather than sharing sensitive genomic data. The approach allows analysis of rare pediatric cancers impossible at single institutions while maintaining formal differential privacy guarantees, discovering 847 novel genotype-phenotype associations including 23 actionable therapeutic targets.",
    "models": "Graph neural networks with attention mechanisms, secure multi-party computation, differential privacy (ε=1.2)",
    "data_sources": "28,500 pediatric cancer cases with whole-genome sequencing from 42 centers in 18 countries, 18TB aggregate data processed locally",
    "metrics": "92.7% accuracy for cancer subtype classification (vs 93.1% centralized, p=0.24; vs 78.3% single-institution, p<0.001), AUROC 0.88 for treatment response, concordance index 0.81 for survival, 847 novel associations discovered including 23 actionable targets, 6.2 days training time vs 14 months for traditional multi-site approvals",
    "needs_manual_review": false
  },
  {
    "title": "Vision-Language Models for Automated Surgical Skill Assessment and Real-Time Feedback in Robotic Surgery",
    "authors": [
      "Zhang Q",
      "Wu T",
      "Chen L",
      "Wang F"
    ],
    "corresponding_author": "Zhang Q",
    "affiliations": [
      "Harvard Medical School",
      "MIT Media Lab",
      "Intuitive Surgical"
    ],
    "abstract": "\nBackground: Surgical skill assessment remains subjective and labor-intensive, limiting feedback during training. \nWe developed vision-language models for automated, objective surgical skill evaluation with real-time feedback \nduring robotic-assisted procedures.\n\nMethods: We collected 12,000 hours of da Vinci surgical system recordings across 6,800 procedures (cholecystectomy, \nprostatectomy, hysterectomy) performed by surgeons ranging from novices (n=45) to experts (n=120). We trained a \nmultimodal transformer architecture combining surgical video analysis (CLIP-based visual encoder) with procedural \nlanguage understanding (GPT-4 based text encoder) to assess technical skills across 12 dimensions including instrument \nhandling, tissue manipulation, and time-motion efficiency. The model generates natural language feedback aligned with \nOSATS (Objective Structured Assessment of Technical Skills) criteria. We validated against expert ratings (3 blinded \nsurgical educators per case) and correlated with clinical outcomes across 2,400 independent test procedures.\n\nResults: The vision-language model achieved 0.89 correlation with expert consensus scores (Spearman's ρ, 95% CI: \n0.87-0.91) across all skill dimensions, with 94% agreement on pass/fail decisions. The model detected critical safety \nevents with 96.4% sensitivity (tissue trauma, bleeding, instrument collisions) and provided contextualized feedback \nwithin 200ms latency enabling real-time intervention. Automated assessment scores predicted postoperative complications \n(AUROC 0.82), operative time efficiency (r=0.76), and surgeon experience level with 91% accuracy. In a pilot training \nprogram with 60 surgical residents, AI-powered feedback accelerated skill acquisition by 34% (p<0.001) measured by \ntime to proficiency, and improved technical skill scores by 2.8 points (5-point scale) compared to traditional \nmentorship alone.\n\nConclusions: Vision-language AI models enable objective, scalable surgical skill assessment with real-time feedback, \npotentially transforming surgical education and quality assurance in the era of robotic surgery.\n        ",
    "url": "https://www.medrxiv.org/content/10.1101/2025.08.15.25307123",
    "published_at": "2025-08-15T00:00:00",
    "source": "medrxiv_2025",
    "doi": "10.1101/2025.08.15.25307123",
    "what_done": "Developed and validated multimodal transformer architecture combining surgical video analysis (CLIP-based) with procedural language understanding (GPT-4 based) to automatically assess technical surgical skills across 12 dimensions. Trained on 12,000 hours of da Vinci surgical recordings from 6,800 procedures performed by surgeons ranging from novices to experts, with validation against expert ratings and correlation with clinical outcomes.",
    "ai_role": "The vision-language AI model provides automated, objective surgical skill assessment with real-time natural language feedback during robotic procedures. It analyzes surgical videos to evaluate technical skills (instrument handling, tissue manipulation, efficiency) aligned with OSATS criteria, detects critical safety events, and generates contextualized feedback within 200ms to enable real-time intervention and accelerate surgical training.",
    "models": "Multimodal transformer architecture with CLIP-based visual encoder and GPT-4 based text encoder",
    "data_sources": "12,000 hours of da Vinci surgical recordings from 6,800 procedures (cholecystectomy, prostatectomy, hysterectomy), 45 novice to 120 expert surgeons, validation on 2,400 independent procedures with expert ratings",
    "metrics": "0.89 correlation with expert consensus (Spearman's ρ, 95% CI: 0.87-0.91), 94% pass/fail agreement, 96.4% sensitivity for critical safety events, 200ms latency for real-time feedback, AUROC 0.82 for complication prediction, r=0.76 for operative efficiency, 91% accuracy for experience level, 34% faster skill acquisition (p<0.001), 2.8-point improvement on 5-point skill scale",
    "needs_manual_review": false
  }
]